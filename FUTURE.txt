# DGE Future Roadmap: Universal Modality Expansion
# =================================================

The DGE methodology (Directed Synergy) solves the "Shared Embedding" problem for text. The next frontier is unifying ALL modalities (Vision, Audio, Sensor Data) into a single, expanding "Brain" that grows distinct cortical regions for each sense while allowing synergy (e.g., Image Captioning) via cross-terms.

## 1. The "Sensorium" (Universal Input Layer)
Current `DGESimpleTransformer` uses a rigid `nn.Embedding` (Integers -> Vector).
**Goal:** Replace the Input Layer with a "Modality Adapter Module".

*   **Logic:** Detect input type (or receive `modality_id`) -> Route to specific projector -> Output `d_model` vector.
*   **Adapters:**
    *   `token_embed`: Standard Text (Current).
    *   `patch_embed`: Vision (ViT-style convolution for 16x16 patches).
    *   `audio_embed`: Audio (Spectrogram projection).
    *   `sensor_embed`: Industrial (Raw float stream projection).

## 2. The "Actuators" (Multi-Head Output)
Current `lm_head` is text-only.
**Goal:** Implement a "Head Bank".

*   **Logic:** Route final hidden state to the task-appropriate head.
*   **Heads:**
    *   `lm_head`: Next token prediction (vocab_size).
    *   `pixel_head`: Image generation (pixel regression or VQ-VAE codebook).
    *   `class_head`: Classification labels (CIFAR-100/ImageNet).
    *   `continuous_head`: Regression (Stock prices, Servo control).

## 3. The "Brain" (Contextual Router++2.0)
Current Bigram Router uses `t-1` (Temporal Context).
**Goal:** Abstract "Context" for spatial/static data.

*   **Spatial Context (Images):** Router context = Neighboring Patches, not just previous time-step.
*   **Modality Tokens:** Insert `[VISION_START]` tokens so the Router immediately switches context from "Language" to "Visual".
*   **Mechanism:** This ensures the Router opens "Visual Quadrants" ($Q_{BL}$) for images and keeps "Language Quadrants" ($Q_{Core}$) protected, preventing interference.

## 4. Training Curriculum (The "Cortical Growth" Plan)
**Step 1: The Linguistic Core (TinyStories)**
*   Train DGE on Text. 
*   Result: A robust Language Model occupying the Core.

**Step 2: The Visual Cortex (Expansion)**
*   Expand Model Width.
*   Train on Image Patches (CIFAR/ImageNet).
*   **Dynamics:** 
    *   Router receives Visual Context.
    *   Router CLOSES Language Gates (Protecting TinyStories).
    *   Router OPENS Expansion Gates ($Q_{BL}$).
    *   Result: A model with distinct Text and Vision regions.

**Step 3: Synergy (Multimodal Tasks)**
*   Train on Image-Caption pairs.
*   **Dynamics:**
    *   Input: Image ($Q_{BL}$) -> Target: Text ($Q_{Core}$).
    *   The Cross-Terms ($Q_{TL} / Q_{BR}$) activate.
    *   The model learns to "Read" visual features into language space.
    *   Result: A VQA (Visual Question Answering) system that hasn't forgotten how to tell stories.

## 5. Unthinkable Modalities
The same logic applies to specific industrial data (e.g., Vibration sensors on machines).
*   **Old Knowledge:** General physics/math learned from text/code.
*   **New Modality:** Raw sensor data.
*   **Synergy:** The model uses its general reasoning (Core) to detect anomalies in the sensor stream (Expansion) via cross-term transfer.

## 6. Router 0: The "IDK Safety Net"
Current routers only output gates (Open/Closed). There is no handling for "all routers are uncertain."
**Goal:** Implement a Base Router (Router 0) as a confidence-aware fallback.

*   **Router 0 (IDK Base):** An always-active residual path that catches inputs when all learned routers have low confidence.
*   **Confidence Aggregation:** A meta-layer that detects "no router is confident" and activates Router 0.
*   **Fallback Action:** Output a special `[IDK]` token or a generic "I don't know" embedding, preventing hallucinations on OOD inputs.
*   **Use Case:** After training TinyStories (Router 1) and Math (Router 2), if questioned about Quantum Physics:
    *   Router 1 (TinyStories): Low confidence (alien topic).
    *   Router 2 (Math): Closed (no formulas).
    *   Router 0 (IDK): Activates and outputs "I don't know" instead of hallucinating.

This transforms DGE from a "Continual Learning" system into a "Self-Aware Curriculum Learner."

## 7. Gradient-Weighted Reservoir Sampling (Optimal Replay Selection)
Current ReplayBuffer uses uniform Reservoir Sampling (all samples equally likely).
**Goal:** Save samples where the Router's gradient is highest.

*   **Concept:** These are the "decision boundary" samples—the ones where the model is uncertain which pathway to use.
*   **Why?** In DGE, the critical knowledge is "when to activate which quadrant." Samples that stress-test this decision are the most valuable for replay.
*   **Implementation:**
    *   During training, track `grad(Router)` magnitude for each sample.
    *   Replace standard Reservoir Sampling with weighted sampling: P(keep) ∝ ||∇Router||.
    *   Store the "hardest" samples, not random samples.
*   **Expected Benefit:** Dramatically reduce replay buffer size while maintaining stability—only the "edge cases" matter.

