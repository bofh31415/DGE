# DGE - Dynamic Growth Engine

**Version:** V 0.18.0 (Package Structure)  
**Branch:** `exp/hierarchical-output`

DGE is a research framework for **continual learning** in neural networks, enabling models to acquire new skills without forgetting previous ones.

## ğŸ“ Project Structure (V0.18.0)

```
Implementation/
â”œâ”€â”€ main.py              # Unified Commander Dashboard
â”œâ”€â”€ version.py           # Version tracking
â”œâ”€â”€ requirements.txt     # Dependencies
â”œâ”€â”€ science.log          # Research & experiment log
â”‚
â”œâ”€â”€ core/                # Core DGE architecture
â”‚   â”œâ”€â”€ model.py         # DGESimpleTransformer
â”‚   â”œâ”€â”€ utils.py         # MoEGatedLinear, HierarchicalOutputHead
â”‚   â”œâ”€â”€ training.py      # DGETrainer
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ cloud/               # Cloud orchestration (RunPod)
â”‚   â”œâ”€â”€ runpod_manager.py
â”‚   â”œâ”€â”€ pod_cleanup.py
â”‚   â””â”€â”€ remote_inference_server.py
â”‚
â”œâ”€â”€ data/                # Data loading & replay
â”‚   â”œâ”€â”€ loader.py
â”‚   â””â”€â”€ replay_buffer.py
â”‚
â”œâ”€â”€ hf/                  # HuggingFace utilities
â”‚   â”œâ”€â”€ repo_manager.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ experiments/         # Experiment scripts
â”‚   â”œâ”€â”€ experiment_lab.py
â”‚   â”œâ”€â”€ run_dge_grand_tour.py
â”‚   â””â”€â”€ run_*.py
â”‚
â”œâ”€â”€ utils/               # General utilities
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ model_manager.py
â”‚
â”œâ”€â”€ tests/               # Unit tests
â””â”€â”€ legacy/              # Archived files
```

## ğŸš€ Quick Start

### Local Development
```bash
cd Implementation
pip install -r requirements.txt
python main.py  # Launch the Unified Commander Dashboard
```

### Run an Experiment
```bash
python -m experiments.run_synergy_experiment
```

### Deploy to RunPod
```bash
python -c "from cloud.runpod_manager import deploy_experiment; deploy_experiment('python -m experiments.run_dge_grand_tour')"
```

## ğŸ“š Import Examples (V0.18.0)
```python
from core.model import DGESimpleTransformer
from core.utils import MoEGatedLinear, HierarchicalOutputHead
from cloud.runpod_manager import deploy_experiment, find_cheapest_gpu
from utils.model_manager import ModelManager
from data.loader import get_dataset
```

## ğŸ”¬ Key Concepts

- **MoEGatedLinear**: Mixture-of-Experts layer with gated expansion
- **HierarchicalOutputHead**: Skill-isolated output heads for additive synergy
- **Expand & Freeze**: Add capacity for new skills while freezing old parameters
- **Router0 IDK**: Base router that outputs uncertainty for unknown inputs

## ğŸ“– Documentation

- **[science.log](science.log)**: Detailed research log with all version changes
- **[RUNPOD_QUICKSTART.md](RUNPOD_QUICKSTART.md)**: Cloud deployment guide

## ğŸ“¦ Dependencies
See [requirements.txt](requirements.txt)

## ğŸ“ License
Research use only. Contact authors for commercial licensing.
