{
  "version": "0.8.0",
  "device": "cuda",
  "config": {
    "vocab_size": 50257,
    "n_layer": 12,
    "max_seq_len": 1024,
    "tinystories_d_model": 384,
    "tinystories_n_head": 6,
    "tinystories_epochs": 1,
    "tinystories_batch_size": 64,
    "tinystories_seq_len": 256,
    "tinystories_max_samples": null,
    "tinystories_lr": 0.0004,
    "expansion_delta": 640,
    "gsm8k_d_model": 1024,
    "gsm8k_n_head": 16,
    "gsm8k_epochs": 3,
    "gsm8k_batch_size": 32,
    "gsm8k_seq_len": 256,
    "gsm8k_max_samples": null,
    "gsm8k_lr": 0.0002,
    "gsm8k_replay_ratio": 0.1,
    "output_dir": "models/tinystories_gsm8k_chain",
    "local_checkpoint_interval": 1000,
    "hf_upload_interval": 5000,
    "git_backup_interval": 5000,
    "gpu_name": "NVIDIA A40",
    "vram_gb": 44.431640625
  },
  "phases": {
    "1_create": {
      "params": 914999424,
      "d_model": 1024
    },
    "3_eval_baseline": {
      "tinystories_ppl": 1.0047244300088636,
      "tinystories_loss": 0.004713304915465414
    },
    "4_expand": {
      "old_d_model": 1024,
      "new_d_model": 1024,
      "old_params": 914999424,
      "new_params": 255129600,
      "added_params": -659869824
    },
    "5_gsm8k": {
      "steps": 34825,
      "time_seconds": 634.3844769001007,
      "replay_ratio": 0.1
    },
    "6_retention": {
      "baseline_ppl": 1.0047244300088636,
      "retention_ppl": 1.0054154834155689,
      "retention_ratio": 0.99931266882388
    },
    "7_gsm8k_eval": {
      "ppl": 10.557013534922373,
      "loss": 2.356790429087097
    }
  },
  "start_time": "2025-12-18T15:28:10.286544",
  "verification": {
    "milestone_gsm8k_final": {
      "error": "Error(s) in loading state_dict for DGESimpleTransformer:\n\tMissing key(s) in state_dict: \"layers.0.w_q.gate_row.router.mlp.0.weight\", \"layers.0.w_q.gate_row.router.mlp.0.bias\", \"layers.0.w_q.gate_row.router.mlp.2.weight\", \"layers.0.w_q.gate_row.router.mlp.2.bias\", \"layers.0.w_q.gate_col.router.mlp.0.weight\", \"layers.0.w_q.gate_col.router.mlp.0.bias\", \"layers.0.w_q.gate_col.router.mlp.2.weight\", \"layers.0.w_q.gate_col.router.mlp.2.bias\", \"layers.0.w_k.gate_row.router.mlp.0.weight\", \"layers.0.w_k.gate_row.router.mlp.0.bias\", \"layers.0.w_k.gate_row.router.mlp.2.weight\", \"layers.0.w_k.gate_row.router.mlp.2.bias\", \"layers.0.w_k.gate_col.router.mlp.0.weight\", \"layers.0.w_k.gate_col.router.mlp.0.bias\", \"layers.0.w_k.gate_col.router.mlp.2.weight\", \"layers.0.w_k.gate_col.router.mlp.2.bias\", \"layers.0.w_v.gate_row.router.mlp.0.weight\", \"layers.0.w_v.gate_row.router.mlp.0.bias\", \"layers.0.w_v.gate_row.router.mlp.2.weight\", \"layers.0.w_v.gate_row.router.mlp.2.bias\", \"layers.0.w_v.gate_col.router.mlp.0.weight\", \"layers.0.w_v.gate_col.router.mlp.0.bias\", \"layers.0.w_v.gate_col.router.mlp.2.weight\", \"layers.0.w_v.gate_col.router.mlp.2.bias\", \"layers.0.w_o.gate_row.router.mlp.0.weight\", \"layers.0.w_o.gate_row.router.mlp.0.bias\", \"layers.0.w_o.gate_row.router.mlp.2.weight\", \"layers.0.w_o.gate_row.router.mlp.2.bias\", \"layers.0.w_o.gate_col.router.mlp.0.weight\", \"layers.0.w_o.gate_col.router.mlp.0.bias\", \"layers.0.w_o.gate_col.router.mlp.2.weight\", \"layers.0.w_o.gate_col.router.mlp.2.bias\", \"layers.0.w_mlp_in.gate_row.router.mlp.0.weight\", \"layers.0.w_mlp_in.gate_row.router.mlp.0.bias\", \"layers.0.w_mlp_in.gate_row.router.mlp.2.weight\", \"layers.0.w_mlp_in.gate_row.router.mlp.2.bias\", \"layers.0.w_mlp_in.gate_col.router.mlp.0.weight\", \"layers.0.w_mlp_in.gate_col.router.mlp.0.bias\", \"layers.0.w_mlp_in.gate_col.router.mlp.2.weight\", \"layers.0.w_mlp_in.gate_col.router.mlp.2.bias\", \"layers.0.w_mlp_out.gate_row.router.mlp.0.weight\", \"layers.0.w_mlp_out.gate_row.router.mlp.0.bias\", \"layers.0.w_mlp_out.gate_row.router.mlp.2.weight\", \"layers.0.w_mlp_out.gate_row.router.mlp.2.bias\", \"layers.0.w_mlp_out.gate_col.router.mlp.0.weight\", \"layers.0.w_mlp_out.gate_col.router.mlp.0.bias\", \"layers.0.w_mlp_out.gate_col.router.mlp.2.weight\", \"layers.0.w_mlp_out.gate_col.router.mlp.2.bias\", \"layers.1.w_q.gate_row.router.mlp.0.weight\", \"layers.1.w_q.gate_row.router.mlp.0.bias\", \"layers.1.w_q.gate_row.router.mlp.2.weight\", \"layers.1.w_q.gate_row.router.mlp.2.bias\", \"layers.1.w_q.gate_col.router.mlp.0.weight\", \"layers.1.w_q.gate_col.router.mlp.0.bias\", \"layers.1.w_q.gate_col.router.mlp.2.weight\", \"layers.1.w_q.gate_col.router.mlp.2.bias\", \"layers.1.w_k.gate_row.router.mlp.0.weight\", \"layers.1.w_k.gate_row.router.mlp.0.bias\", \"layers.1.w_k.gate_row.router.mlp.2.weight\", \"layers.1.w_k.gate_row.router.mlp.2.bias\", \"layers.1.w_k.gate_col.router.mlp.0.weight\", \"layers.1.w_k.gate_col.router.mlp.0.bias\", \"layers.1.w_k.gate_col.router.mlp.2.weight\", \"layers.1.w_k.gate_col.router.mlp.2.bias\", \"layers.1.w_v.gate_row.router.mlp.0.weight\", \"layers.1.w_v.gate_row.router.mlp.0.bias\", \"layers.1.w_v.gate_row.router.mlp.2.weight\", \"layers.1.w_v.gate_row.router.mlp.2.bias\", \"layers.1.w_v.gate_col.router.mlp.0.weight\", \"layers.1.w_v.gate_col.router.mlp.0.bias\", \"layers.1.w_v.gate_col.router.mlp.2.weight\", \"layers.1.w_v.gate_col.router.mlp.2.bias\", \"layers.1.w_o.gate_row.router.mlp.0.weight\", \"layers.1.w_o.gate_row.router.mlp.0.bias\", \"layers.1.w_o.gate_row.router.mlp.2.weight\", \"layers.1.w_o.gate_row.router.mlp.2.bias\", \"layers.1.w_o.gate_col.router.mlp.0.weight\", \"layers.1.w_o.gate_col.router.mlp.0.bias\", \"layers.1.w_o.gate_col.router.mlp.2.weight\", \"layers.1.w_o.gate_col.router.mlp.2.bias\", \"layers.1.w_mlp_in.gate_row.router.mlp.0.weight\", \"layers.1.w_mlp_in.gate_row.router.mlp.0.bias\", \"layers.1.w_mlp_in.gate_row.router.mlp.2.weight\", \"layers.1.w_mlp_in.gate_row.router.mlp.2.bias\", \"layers.1.w_mlp_in.gate_col.router.mlp.0.weight\", \"layers.1.w_mlp_in.gate_col.router.mlp.0.bias\", \"layers.1.w_mlp_in.gate_col.router.mlp.2.weight\", \"layers.1.w_mlp_in.gate_col.router.mlp.2.bias\", \"layers.1.w_mlp_out.gate_row.router.mlp.0.weight\", \"layers.1.w_mlp_out.gate_row.router.mlp.0.bias\", \"layers.1.w_mlp_out.gate_row.router.mlp.2.weight\", \"layers.1.w_mlp_out.gate_row.router.mlp.2.bias\", \"layers.1.w_mlp_out.gate_col.router.mlp.0.weight\", \"layers.1.w_mlp_out.gate_col.router.mlp.0.bias\", \"layers.1.w_mlp_out.gate_col.router.mlp.2.weight\", \"layers.1.w_mlp_out.gate_col.router.mlp.2.bias\", \"layers.2.w_q.gate_row.router.mlp.0.weight\", \"layers.2.w_q.gate_row.router.mlp.0.bias\", \"layers.2.w_q.gate_row.router.mlp.2.weight\", \"layers.2.w_q.gate_row.router.mlp.2.bias\", \"layers.2.w_q.gate_col.router.mlp.0.weight\", \"layers.2.w_q.gate_col.router.mlp.0.bias\", \"layers.2.w_q.gate_col.router.mlp.2.weight\", \"layers.2.w_q.gate_col.router.mlp.2.bias\", \"layers.2.w_k.gate_row.router.mlp.0.weight\", \"layers.2.w_k.gate_row.router.mlp.0.bias\", \"layers.2.w_k.gate_row.router.mlp.2.weight\", \"layers.2.w_k.gate_row.router.mlp.2.bias\", \"layers.2.w_k.gate_col.router.mlp.0.weight\", \"layers.2.w_k.gate_col.router.mlp.0.bias\", \"layers.2.w_k.gate_col.router.mlp.2.weight\", \"layers.2.w_k.gate_col.router.mlp.2.bias\", \"layers.2.w_v.gate_row.router.mlp.0.weight\", \"layers.2.w_v.gate_row.router.mlp.0.bias\", \"layers.2.w_v.gate_row.router.mlp.2.weight\", \"layers.2.w_v.gate_row.router.mlp.2.bias\", \"layers.2.w_v.gate_col.router.mlp.0.weight\", \"layers.2.w_v.gate_col.router.mlp.0.bias\", \"layers.2.w_v.gate_col.router.mlp.2.weight\", \"layers.2.w_v.gate_col.router.mlp.2.bias\", \"layers.2.w_o.gate_row.router.mlp.0.weight\", \"layers.2.w_o.gate_row.router.mlp.0.bias\", \"layers.2.w_o.gate_row.router.mlp.2.weight\", \"layers.2.w_o.gate_row.router.mlp.2.bias\", \"layers.2.w_o.gate_col.router.mlp.0.weight\", \"layers.2.w_o.gate_col.router.mlp.0.bias\", \"layers.2.w_o.gate_col.router.mlp.2.weight\", \"layers.2.w_o.gate_col.router.mlp.2.bias\", \"layers.2.w_mlp_in.gate_row.router.mlp.0.weight\", \"layers.2.w_mlp_in.gate_row.router.mlp.0.bias\", \"layers.2.w_mlp_in.gate_row.router.mlp.2.weight\", \"layers.2.w_mlp_in.gate_row.router.mlp.2.bias\", \"layers.2.w_mlp_in.gate_col.router.mlp.0.weight\", \"layers.2.w_mlp_in.gate_col.router.mlp.0.bias\", \"layers.2.w_mlp_in.gate_col.router.mlp.2.weight\", \"layers.2.w_mlp_in.gate_col.router.mlp.2.bias\", \"layers.2.w_mlp_out.gate_row.router.mlp.0.weight\", \"layers.2.w_mlp_out.gate_row.router.mlp.0.bias\", \"layers.2.w_mlp_out.gate_row.router.mlp.2.weight\", \"layers.2.w_mlp_out.gate_row.router.mlp.2.bias\", \"layers.2.w_mlp_out.gate_col.router.mlp.0.weight\", \"layers.2.w_mlp_out.gate_col.router.mlp.0.bias\", \"layers.2.w_mlp_out.gate_col.router.mlp.2.weight\", \"layers.2.w_mlp_out.gate_col.router.mlp.2.bias\", \"layers.3.w_q.gate_row.router.mlp.0.weight\", \"layers.3.w_q.gate_row.router.mlp.0.bias\", \"layers.3.w_q.gate_row.router.mlp.2.weight\", \"layers.3.w_q.gate_row.router.mlp.2.bias\", \"layers.3.w_q.gate_col.router.mlp.0.weight\", \"layers.3.w_q.gate_col.router.mlp.0.bias\", \"layers.3.w_q.gate_col.router.mlp.2.weight\", \"layers.3.w_q.gate_col.router.mlp.2.bias\", \"layers.3.w_k.gate_row.router.mlp.0.weight\", \"layers.3.w_k.gate_row.router.mlp.0.bias\", \"layers.3.w_k.gate_row.router.mlp.2.weight\", \"layers.3.w_k.gate_row.router.mlp.2.bias\", \"layers.3.w_k.gate_col.router.mlp.0.weight\", \"layers.3.w_k.gate_col.router.mlp.0.bias\", \"layers.3.w_k.gate_col.router.mlp.2.weight\", \"layers.3.w_k.gate_col.router.mlp.2.bias\", \"layers.3.w_v.gate_row.router.mlp.0.weight\", \"layers.3.w_v.gate_row.router.mlp.0.bias\", \"layers.3.w_v.gate_row.router.mlp.2.weight\", \"layers.3.w_v.gate_row.router.mlp.2.bias\", \"layers.3.w_v.gate_col.router.mlp.0.weight\", \"layers.3.w_v.gate_col.router.mlp.0.bias\", \"layers.3.w_v.gate_col.router.mlp.2.weight\", \"layers.3.w_v.gate_col.router.mlp.2.bias\", \"layers.3.w_o.gate_row.router.mlp.0.weight\", \"layers.3.w_o.gate_row.router.mlp.0.bias\", \"layers.3.w_o.gate_row.router.mlp.2.weight\", \"layers.3.w_o.gate_row.router.mlp.2.bias\", \"layers.3.w_o.gate_col.router.mlp.0.weight\", \"layers.3.w_o.gate_col.router.mlp.0.bias\", \"layers.3.w_o.gate_col.router.mlp.2.weight\", \"layers.3.w_o.gate_col.router.mlp.2.bias\", \"layers.3.w_mlp_in.gate_row.router.mlp.0.weight\", \"layers.3.w_mlp_in.gate_row.router.mlp.0.bias\", \"layers.3.w_mlp_in.gate_row.router.mlp.2.weight\", \"layers.3.w_mlp_in.gate_row.router.mlp.2.bias\", \"layers.3.w_mlp_in.gate_col.router.mlp.0.weight\", \"layers.3.w_mlp_in.gate_col.router.mlp.0.bias\", \"layers.3.w_mlp_in.gate_col.router.mlp.2.weight\", \"layers.3.w_mlp_in.gate_col.router.mlp.2.bias\", \"layers.3.w_mlp_out.gate_row.router.mlp.0.weight\", \"layers.3.w_mlp_out.gate_row.router.mlp.0.bias\", \"layers.3.w_mlp_out.gate_row.router.mlp.2.weight\", \"layers.3.w_mlp_out.gate_row.router.mlp.2.bias\", \"layers.3.w_mlp_out.gate_col.router.mlp.0.weight\", \"layers.3.w_mlp_out.gate_col.router.mlp.0.bias\", \"layers.3.w_mlp_out.gate_col.router.mlp.2.weight\", \"layers.3.w_mlp_out.gate_col.router.mlp.2.bias\", \"layers.4.w_q.gate_row.router.mlp.0.weight\", \"layers.4.w_q.gate_row.router.mlp.0.bias\", \"layers.4.w_q.gate_row.router.mlp.2.weight\", \"layers.4.w_q.gate_row.router.mlp.2.bias\", \"layers.4.w_q.gate_col.router.mlp.0.weight\", \"layers.4.w_q.gate_col.router.mlp.0.bias\", \"layers.4.w_q.gate_col.router.mlp.2.weight\", \"layers.4.w_q.gate_col.router.mlp.2.bias\", \"layers.4.w_k.gate_row.router.mlp.0.weight\", \"layers.4.w_k.gate_row.router.mlp.0.bias\", \"layers.4.w_k.gate_row.router.mlp.2.weight\", \"layers.4.w_k.gate_row.router.mlp.2.bias\", \"layers.4.w_k.gate_col.router.mlp.0.weight\", \"layers.4.w_k.gate_col.router.mlp.0.bias\", \"layers.4.w_k.gate_col.router.mlp.2.weight\", \"layers.4.w_k.gate_col.router.mlp.2.bias\", \"layers.4.w_v.gate_row.router.mlp.0.weight\", \"layers.4.w_v.gate_row.router.mlp.0.bias\", \"layers.4.w_v.gate_row.router.mlp.2.weight\", \"layers.4.w_v.gate_row.router.mlp.2.bias\", \"layers.4.w_v.gate_col.router.mlp.0.weight\", \"layers.4.w_v.gate_col.router.mlp.0.bias\", \"layers.4.w_v.gate_col.router.mlp.2.weight\", \"layers.4.w_v.gate_col.router.mlp.2.bias\", \"layers.4.w_o.gate_row.router.mlp.0.weight\", \"layers.4.w_o.gate_row.router.mlp.0.bias\", \"layers.4.w_o.gate_row.router.mlp.2.weight\", \"layers.4.w_o.gate_row.router.mlp.2.bias\", \"layers.4.w_o.gate_col.router.mlp.0.weight\", \"layers.4.w_o.gate_col.router.mlp.0.bias\", \"layers.4.w_o.gate_col.router.mlp.2.weight\", \"layers.4.w_o.gate_col.router.mlp.2.bias\", \"layers.4.w_mlp_in.gate_row.router.mlp.0.weight\", \"layers.4.w_mlp_in.gate_row.router.mlp.0.bias\", \"layers.4.w_mlp_in.gate_row.router.mlp.2.weight\", \"layers.4.w_mlp_in.gate_row.router.mlp.2.bias\", \"layers.4.w_mlp_in.gate_col.router.mlp.0.weight\", \"layers.4.w_mlp_in.gate_col.router.mlp.0.bias\", \"layers.4.w_mlp_in.gate_col.router.mlp.2.weight\", \"layers.4.w_mlp_in.gate_col.router.mlp.2.bias\", \"layers.4.w_mlp_out.gate_row.router.mlp.0.weight\", \"layers.4.w_mlp_out.gate_row.router.mlp.0.bias\", \"layers.4.w_mlp_out.gate_row.router.mlp.2.weight\", \"layers.4.w_mlp_out.gate_row.router.mlp.2.bias\", \"layers.4.w_mlp_out.gate_col.router.mlp.0.weight\", \"layers.4.w_mlp_out.gate_col.router.mlp.0.bias\", \"layers.4.w_mlp_out.gate_col.router.mlp.2.weight\", \"layers.4.w_mlp_out.gate_col.router.mlp.2.bias\", \"layers.5.w_q.gate_row.router.mlp.0.weight\", \"layers.5.w_q.gate_row.router.mlp.0.bias\", \"layers.5.w_q.gate_row.router.mlp.2.weight\", \"layers.5.w_q.gate_row.router.mlp.2.bias\", \"layers.5.w_q.gate_col.router.mlp.0.weight\", \"layers.5.w_q.gate_col.router.mlp.0.bias\", \"layers.5.w_q.gate_col.router.mlp.2.weight\", \"layers.5.w_q.gate_col.router.mlp.2.bias\", \"layers.5.w_k.gate_row.router.mlp.0.weight\", \"layers.5.w_k.gate_row.router.mlp.0.bias\", \"layers.5.w_k.gate_row.router.mlp.2.weight\", \"layers.5.w_k.gate_row.router.mlp.2.bias\", \"layers.5.w_k.gate_col.router.mlp.0.weight\", \"layers.5.w_k.gate_col.router.mlp.0.bias\", \"layers.5.w_k.gate_col.router.mlp.2.weight\", \"layers.5.w_k.gate_col.router.mlp.2.bias\", \"layers.5.w_v.gate_row.router.mlp.0.weight\", \"layers.5.w_v.gate_row.router.mlp.0.bias\", \"layers.5.w_v.gate_row.router.mlp.2.weight\", \"layers.5.w_v.gate_row.router.mlp.2.bias\", \"layers.5.w_v.gate_col.router.mlp.0.weight\", \"layers.5.w_v.gate_col.router.mlp.0.bias\", \"layers.5.w_v.gate_col.router.mlp.2.weight\", \"layers.5.w_v.gate_col.router.mlp.2.bias\", \"layers.5.w_o.gate_row.router.mlp.0.weight\", \"layers.5.w_o.gate_row.router.mlp.0.bias\", \"layers.5.w_o.gate_row.router.mlp.2.weight\", \"layers.5.w_o.gate_row.router.mlp.2.bias\", \"layers.5.w_o.gate_col.router.mlp.0.weight\", \"layers.5.w_o.gate_col.router.mlp.0.bias\", \"layers.5.w_o.gate_col.router.mlp.2.weight\", \"layers.5.w_o.gate_col.router.mlp.2.bias\", \"layers.5.w_mlp_in.gate_row.router.mlp.0.weight\", \"layers.5.w_mlp_in.gate_row.router.mlp.0.bias\", \"layers.5.w_mlp_in.gate_row.router.mlp.2.weight\", \"layers.5.w_mlp_in.gate_row.router.mlp.2.bias\", \"layers.5.w_mlp_in.gate_col.router.mlp.0.weight\", \"layers.5.w_mlp_in.gate_col.router.mlp.0.bias\", \"layers.5.w_mlp_in.gate_col.router.mlp.2.weight\", \"layers.5.w_mlp_in.gate_col.router.mlp.2.bias\", \"layers.5.w_mlp_out.gate_row.router.mlp.0.weight\", \"layers.5.w_mlp_out.gate_row.router.mlp.0.bias\", \"layers.5.w_mlp_out.gate_row.router.mlp.2.weight\", \"layers.5.w_mlp_out.gate_row.router.mlp.2.bias\", \"layers.5.w_mlp_out.gate_col.router.mlp.0.weight\", \"layers.5.w_mlp_out.gate_col.router.mlp.0.bias\", \"layers.5.w_mlp_out.gate_col.router.mlp.2.weight\", \"layers.5.w_mlp_out.gate_col.router.mlp.2.bias\", \"layers.6.w_q.gate_row.router.mlp.0.weight\", \"layers.6.w_q.gate_row.router.mlp.0.bias\", \"layers.6.w_q.gate_row.router.mlp.2.weight\", \"layers.6.w_q.gate_row.router.mlp.2.bias\", \"layers.6.w_q.gate_col.router.mlp.0.weight\", \"layers.6.w_q.gate_col.router.mlp.0.bias\", \"layers.6.w_q.gate_col.router.mlp.2.weight\", \"layers.6.w_q.gate_col.router.mlp.2.bias\", \"layers.6.w_k.gate_row.router.mlp.0.weight\", \"layers.6.w_k.gate_row.router.mlp.0.bias\", \"layers.6.w_k.gate_row.router.mlp.2.weight\", \"layers.6.w_k.gate_row.router.mlp.2.bias\", \"layers.6.w_k.gate_col.router.mlp.0.weight\", \"layers.6.w_k.gate_col.router.mlp.0.bias\", \"layers.6.w_k.gate_col.router.mlp.2.weight\", \"layers.6.w_k.gate_col.router.mlp.2.bias\", \"layers.6.w_v.gate_row.router.mlp.0.weight\", \"layers.6.w_v.gate_row.router.mlp.0.bias\", \"layers.6.w_v.gate_row.router.mlp.2.weight\", \"layers.6.w_v.gate_row.router.mlp.2.bias\", \"layers.6.w_v.gate_col.router.mlp.0.weight\", \"layers.6.w_v.gate_col.router.mlp.0.bias\", \"layers.6.w_v.gate_col.router.mlp.2.weight\", \"layers.6.w_v.gate_col.router.mlp.2.bias\", \"layers.6.w_o.gate_row.router.mlp.0.weight\", \"layers.6.w_o.gate_row.router.mlp.0.bias\", \"layers.6.w_o.gate_row.router.mlp.2.weight\", \"layers.6.w_o.gate_row.router.mlp.2.bias\", \"layers.6.w_o.gate_col.router.mlp.0.weight\", \"layers.6.w_o.gate_col.router.mlp.0.bias\", \"layers.6.w_o.gate_col.router.mlp.2.weight\", \"layers.6.w_o.gate_col.router.mlp.2.bias\", \"layers.6.w_mlp_in.gate_row.router.mlp.0.weight\", \"layers.6.w_mlp_in.gate_row.router.mlp.0.bias\", \"layers.6.w_mlp_in.gate_row.router.mlp.2.weight\", \"layers.6.w_mlp_in.gate_row.router.mlp.2.bias\", \"layers.6.w_mlp_in.gate_col.router.mlp.0.weight\", \"layers.6.w_mlp_in.gate_col.router.mlp.0.bias\", \"layers.6.w_mlp_in.gate_col.router.mlp.2.weight\", \"layers.6.w_mlp_in.gate_col.router.mlp.2.bias\", \"layers.6.w_mlp_out.gate_row.router.mlp.0.weight\", \"layers.6.w_mlp_out.gate_row.router.mlp.0.bias\", \"layers.6.w_mlp_out.gate_row.router.mlp.2.weight\", \"layers.6.w_mlp_out.gate_row.router.mlp.2.bias\", \"layers.6.w_mlp_out.gate_col.router.mlp.0.weight\", \"layers.6.w_mlp_out.gate_col.router.mlp.0.bias\", \"layers.6.w_mlp_out.gate_col.router.mlp.2.weight\", \"layers.6.w_mlp_out.gate_col.router.mlp.2.bias\", \"layers.7.w_q.gate_row.router.mlp.0.weight\", \"layers.7.w_q.gate_row.router.mlp.0.bias\", \"layers.7.w_q.gate_row.router.mlp.2.weight\", \"layers.7.w_q.gate_row.router.mlp.2.bias\", \"layers.7.w_q.gate_col.router.mlp.0.weight\", \"layers.7.w_q.gate_col.router.mlp.0.bias\", \"layers.7.w_q.gate_col.router.mlp.2.weight\", \"layers.7.w_q.gate_col.router.mlp.2.bias\", \"layers.7.w_k.gate_row.router.mlp.0.weight\", \"layers.7.w_k.gate_row.router.mlp.0.bias\", \"layers.7.w_k.gate_row.router.mlp.2.weight\", \"layers.7.w_k.gate_row.router.mlp.2.bias\", \"layers.7.w_k.gate_col.router.mlp.0.weight\", \"layers.7.w_k.gate_col.router.mlp.0.bias\", \"layers.7.w_k.gate_col.router.mlp.2.weight\", \"layers.7.w_k.gate_col.router.mlp.2.bias\", \"layers.7.w_v.gate_row.router.mlp.0.weight\", \"layers.7.w_v.gate_row.router.mlp.0.bias\", \"layers.7.w_v.gate_row.router.mlp.2.weight\", \"layers.7.w_v.gate_row.router.mlp.2.bias\", \"layers.7.w_v.gate_col.router.mlp.0.weight\", \"layers.7.w_v.gate_col.router.mlp.0.bias\", \"layers.7.w_v.gate_col.router.mlp.2.weight\", \"layers.7.w_v.gate_col.router.mlp.2.bias\", \"layers.7.w_o.gate_row.router.mlp.0.weight\", \"layers.7.w_o.gate_row.router.mlp.0.bias\", \"layers.7.w_o.gate_row.router.mlp.2.weight\", \"layers.7.w_o.gate_row.router.mlp.2.bias\", \"layers.7.w_o.gate_col.router.mlp.0.weight\", \"layers.7.w_o.gate_col.router.mlp.0.bias\", \"layers.7.w_o.gate_col.router.mlp.2.weight\", \"layers.7.w_o.gate_col.router.mlp.2.bias\", \"layers.7.w_mlp_in.gate_row.router.mlp.0.weight\", \"layers.7.w_mlp_in.gate_row.router.mlp.0.bias\", \"layers.7.w_mlp_in.gate_row.router.mlp.2.weight\", \"layers.7.w_mlp_in.gate_row.router.mlp.2.bias\", \"layers.7.w_mlp_in.gate_col.router.mlp.0.weight\", \"layers.7.w_mlp_in.gate_col.router.mlp.0.bias\", \"layers.7.w_mlp_in.gate_col.router.mlp.2.weight\", \"layers.7.w_mlp_in.gate_col.router.mlp.2.bias\", \"layers.7.w_mlp_out.gate_row.router.mlp.0.weight\", \"layers.7.w_mlp_out.gate_row.router.mlp.0.bias\", \"layers.7.w_mlp_out.gate_row.router.mlp.2.weight\", \"layers.7.w_mlp_out.gate_row.router.mlp.2.bias\", \"layers.7.w_mlp_out.gate_col.router.mlp.0.weight\", \"layers.7.w_mlp_out.gate_col.router.mlp.0.bias\", \"layers.7.w_mlp_out.gate_col.router.mlp.2.weight\", \"layers.7.w_mlp_out.gate_col.router.mlp.2.bias\", \"layers.8.w_q.gate_row.router.mlp.0.weight\", \"layers.8.w_q.gate_row.router.mlp.0.bias\", \"layers.8.w_q.gate_row.router.mlp.2.weight\", \"layers.8.w_q.gate_row.router.mlp.2.bias\", \"layers.8.w_q.gate_col.router.mlp.0.weight\", \"layers.8.w_q.gate_col.router.mlp.0.bias\", \"layers.8.w_q.gate_col.router.mlp.2.weight\", \"layers.8.w_q.gate_col.router.mlp.2.bias\", \"layers.8.w_k.gate_row.router.mlp.0.weight\", \"layers.8.w_k.gate_row.router.mlp.0.bias\", \"layers.8.w_k.gate_row.router.mlp.2.weight\", \"layers.8.w_k.gate_row.router.mlp.2.bias\", \"layers.8.w_k.gate_col.router.mlp.0.weight\", \"layers.8.w_k.gate_col.router.mlp.0.bias\", \"layers.8.w_k.gate_col.router.mlp.2.weight\", \"layers.8.w_k.gate_col.router.mlp.2.bias\", \"layers.8.w_v.gate_row.router.mlp.0.weight\", \"layers.8.w_v.gate_row.router.mlp.0.bias\", \"layers.8.w_v.gate_row.router.mlp.2.weight\", \"layers.8.w_v.gate_row.router.mlp.2.bias\", \"layers.8.w_v.gate_col.router.mlp.0.weight\", \"layers.8.w_v.gate_col.router.mlp.0.bias\", \"layers.8.w_v.gate_col.router.mlp.2.weight\", \"layers.8.w_v.gate_col.router.mlp.2.bias\", \"layers.8.w_o.gate_row.router.mlp.0.weight\", \"layers.8.w_o.gate_row.router.mlp.0.bias\", \"layers.8.w_o.gate_row.router.mlp.2.weight\", \"layers.8.w_o.gate_row.router.mlp.2.bias\", \"layers.8.w_o.gate_col.router.mlp.0.weight\", \"layers.8.w_o.gate_col.router.mlp.0.bias\", \"layers.8.w_o.gate_col.router.mlp.2.weight\", \"layers.8.w_o.gate_col.router.mlp.2.bias\", \"layers.8.w_mlp_in.gate_row.router.mlp.0.weight\", \"layers.8.w_mlp_in.gate_row.router.mlp.0.bias\", \"layers.8.w_mlp_in.gate_row.router.mlp.2.weight\", \"layers.8.w_mlp_in.gate_row.router.mlp.2.bias\", \"layers.8.w_mlp_in.gate_col.router.mlp.0.weight\", \"layers.8.w_mlp_in.gate_col.router.mlp.0.bias\", \"layers.8.w_mlp_in.gate_col.router.mlp.2.weight\", \"layers.8.w_mlp_in.gate_col.router.mlp.2.bias\", \"layers.8.w_mlp_out.gate_row.router.mlp.0.weight\", \"layers.8.w_mlp_out.gate_row.router.mlp.0.bias\", \"layers.8.w_mlp_out.gate_row.router.mlp.2.weight\", \"layers.8.w_mlp_out.gate_row.router.mlp.2.bias\", \"layers.8.w_mlp_out.gate_col.router.mlp.0.weight\", \"layers.8.w_mlp_out.gate_col.router.mlp.0.bias\", \"layers.8.w_mlp_out.gate_col.router.mlp.2.weight\", \"layers.8.w_mlp_out.gate_col.router.mlp.2.bias\", \"layers.9.w_q.gate_row.router.mlp.0.weight\", \"layers.9.w_q.gate_row.router.mlp.0.bias\", \"layers.9.w_q.gate_row.router.mlp.2.weight\", \"layers.9.w_q.gate_row.router.mlp.2.bias\", \"layers.9.w_q.gate_col.router.mlp.0.weight\", \"layers.9.w_q.gate_col.router.mlp.0.bias\", \"layers.9.w_q.gate_col.router.mlp.2.weight\", \"layers.9.w_q.gate_col.router.mlp.2.bias\", \"layers.9.w_k.gate_row.router.mlp.0.weight\", \"layers.9.w_k.gate_row.router.mlp.0.bias\", \"layers.9.w_k.gate_row.router.mlp.2.weight\", \"layers.9.w_k.gate_row.router.mlp.2.bias\", \"layers.9.w_k.gate_col.router.mlp.0.weight\", \"layers.9.w_k.gate_col.router.mlp.0.bias\", \"layers.9.w_k.gate_col.router.mlp.2.weight\", \"layers.9.w_k.gate_col.router.mlp.2.bias\", \"layers.9.w_v.gate_row.router.mlp.0.weight\", \"layers.9.w_v.gate_row.router.mlp.0.bias\", \"layers.9.w_v.gate_row.router.mlp.2.weight\", \"layers.9.w_v.gate_row.router.mlp.2.bias\", \"layers.9.w_v.gate_col.router.mlp.0.weight\", \"layers.9.w_v.gate_col.router.mlp.0.bias\", \"layers.9.w_v.gate_col.router.mlp.2.weight\", \"layers.9.w_v.gate_col.router.mlp.2.bias\", \"layers.9.w_o.gate_row.router.mlp.0.weight\", \"layers.9.w_o.gate_row.router.mlp.0.bias\", \"layers.9.w_o.gate_row.router.mlp.2.weight\", \"layers.9.w_o.gate_row.router.mlp.2.bias\", \"layers.9.w_o.gate_col.router.mlp.0.weight\", \"layers.9.w_o.gate_col.router.mlp.0.bias\", \"layers.9.w_o.gate_col.router.mlp.2.weight\", \"layers.9.w_o.gate_col.router.mlp.2.bias\", \"layers.9.w_mlp_in.gate_row.router.mlp.0.weight\", \"layers.9.w_mlp_in.gate_row.router.mlp.0.bias\", \"layers.9.w_mlp_in.gate_row.router.mlp.2.weight\", \"layers.9.w_mlp_in.gate_row.router.mlp.2.bias\", \"layers.9.w_mlp_in.gate_col.router.mlp.0.weight\", \"layers.9.w_mlp_in.gate_col.router.mlp.0.bias\", \"layers.9.w_mlp_in.gate_col.router.mlp.2.weight\", \"layers.9.w_mlp_in.gate_col.router.mlp.2.bias\", \"layers.9.w_mlp_out.gate_row.router.mlp.0.weight\", \"layers.9.w_mlp_out.gate_row.router.mlp.0.bias\", \"layers.9.w_mlp_out.gate_row.router.mlp.2.weight\", \"layers.9.w_mlp_out.gate_row.router.mlp.2.bias\", \"layers.9.w_mlp_out.gate_col.router.mlp.0.weight\", \"layers.9.w_mlp_out.gate_col.router.mlp.0.bias\", \"layers.9.w_mlp_out.gate_col.router.mlp.2.weight\", \"layers.9.w_mlp_out.gate_col.router.mlp.2.bias\", \"layers.10.w_q.gate_row.router.mlp.0.weight\", \"layers.10.w_q.gate_row.router.mlp.0.bias\", \"layers.10.w_q.gate_row.router.mlp.2.weight\", \"layers.10.w_q.gate_row.router.mlp.2.bias\", \"layers.10.w_q.gate_col.router.mlp.0.weight\", \"layers.10.w_q.gate_col.router.mlp.0.bias\", \"layers.10.w_q.gate_col.router.mlp.2.weight\", \"layers.10.w_q.gate_col.router.mlp.2.bias\", \"layers.10.w_k.gate_row.router.mlp.0.weight\", \"layers.10.w_k.gate_row.router.mlp.0.bias\", \"layers.10.w_k.gate_row.router.mlp.2.weight\", \"layers.10.w_k.gate_row.router.mlp.2.bias\", \"layers.10.w_k.gate_col.router.mlp.0.weight\", \"layers.10.w_k.gate_col.router.mlp.0.bias\", \"layers.10.w_k.gate_col.router.mlp.2.weight\", \"layers.10.w_k.gate_col.router.mlp.2.bias\", \"layers.10.w_v.gate_row.router.mlp.0.weight\", \"layers.10.w_v.gate_row.router.mlp.0.bias\", \"layers.10.w_v.gate_row.router.mlp.2.weight\", \"layers.10.w_v.gate_row.router.mlp.2.bias\", \"layers.10.w_v.gate_col.router.mlp.0.weight\", \"layers.10.w_v.gate_col.router.mlp.0.bias\", \"layers.10.w_v.gate_col.router.mlp.2.weight\", \"layers.10.w_v.gate_col.router.mlp.2.bias\", \"layers.10.w_o.gate_row.router.mlp.0.weight\", \"layers.10.w_o.gate_row.router.mlp.0.bias\", \"layers.10.w_o.gate_row.router.mlp.2.weight\", \"layers.10.w_o.gate_row.router.mlp.2.bias\", \"layers.10.w_o.gate_col.router.mlp.0.weight\", \"layers.10.w_o.gate_col.router.mlp.0.bias\", \"layers.10.w_o.gate_col.router.mlp.2.weight\", \"layers.10.w_o.gate_col.router.mlp.2.bias\", \"layers.10.w_mlp_in.gate_row.router.mlp.0.weight\", \"layers.10.w_mlp_in.gate_row.router.mlp.0.bias\", \"layers.10.w_mlp_in.gate_row.router.mlp.2.weight\", \"layers.10.w_mlp_in.gate_row.router.mlp.2.bias\", \"layers.10.w_mlp_in.gate_col.router.mlp.0.weight\", \"layers.10.w_mlp_in.gate_col.router.mlp.0.bias\", \"layers.10.w_mlp_in.gate_col.router.mlp.2.weight\", \"layers.10.w_mlp_in.gate_col.router.mlp.2.bias\", \"layers.10.w_mlp_out.gate_row.router.mlp.0.weight\", \"layers.10.w_mlp_out.gate_row.router.mlp.0.bias\", \"layers.10.w_mlp_out.gate_row.router.mlp.2.weight\", \"layers.10.w_mlp_out.gate_row.router.mlp.2.bias\", \"layers.10.w_mlp_out.gate_col.router.mlp.0.weight\", \"layers.10.w_mlp_out.gate_col.router.mlp.0.bias\", \"layers.10.w_mlp_out.gate_col.router.mlp.2.weight\", \"layers.10.w_mlp_out.gate_col.router.mlp.2.bias\", \"layers.11.w_q.gate_row.router.mlp.0.weight\", \"layers.11.w_q.gate_row.router.mlp.0.bias\", \"layers.11.w_q.gate_row.router.mlp.2.weight\", \"layers.11.w_q.gate_row.router.mlp.2.bias\", \"layers.11.w_q.gate_col.router.mlp.0.weight\", \"layers.11.w_q.gate_col.router.mlp.0.bias\", \"layers.11.w_q.gate_col.router.mlp.2.weight\", \"layers.11.w_q.gate_col.router.mlp.2.bias\", \"layers.11.w_k.gate_row.router.mlp.0.weight\", \"layers.11.w_k.gate_row.router.mlp.0.bias\", \"layers.11.w_k.gate_row.router.mlp.2.weight\", \"layers.11.w_k.gate_row.router.mlp.2.bias\", \"layers.11.w_k.gate_col.router.mlp.0.weight\", \"layers.11.w_k.gate_col.router.mlp.0.bias\", \"layers.11.w_k.gate_col.router.mlp.2.weight\", \"layers.11.w_k.gate_col.router.mlp.2.bias\", \"layers.11.w_v.gate_row.router.mlp.0.weight\", \"layers.11.w_v.gate_row.router.mlp.0.bias\", \"layers.11.w_v.gate_row.router.mlp.2.weight\", \"layers.11.w_v.gate_row.router.mlp.2.bias\", \"layers.11.w_v.gate_col.router.mlp.0.weight\", \"layers.11.w_v.gate_col.router.mlp.0.bias\", \"layers.11.w_v.gate_col.router.mlp.2.weight\", \"layers.11.w_v.gate_col.router.mlp.2.bias\", \"layers.11.w_o.gate_row.router.mlp.0.weight\", \"layers.11.w_o.gate_row.router.mlp.0.bias\", \"layers.11.w_o.gate_row.router.mlp.2.weight\", \"layers.11.w_o.gate_row.router.mlp.2.bias\", \"layers.11.w_o.gate_col.router.mlp.0.weight\", \"layers.11.w_o.gate_col.router.mlp.0.bias\", \"layers.11.w_o.gate_col.router.mlp.2.weight\", \"layers.11.w_o.gate_col.router.mlp.2.bias\", \"layers.11.w_mlp_in.gate_row.router.mlp.0.weight\", \"layers.11.w_mlp_in.gate_row.router.mlp.0.bias\", \"layers.11.w_mlp_in.gate_row.router.mlp.2.weight\", \"layers.11.w_mlp_in.gate_row.router.mlp.2.bias\", \"layers.11.w_mlp_in.gate_col.router.mlp.0.weight\", \"layers.11.w_mlp_in.gate_col.router.mlp.0.bias\", \"layers.11.w_mlp_in.gate_col.router.mlp.2.weight\", \"layers.11.w_mlp_in.gate_col.router.mlp.2.bias\", \"layers.11.w_mlp_out.gate_row.router.mlp.0.weight\", \"layers.11.w_mlp_out.gate_row.router.mlp.0.bias\", \"layers.11.w_mlp_out.gate_row.router.mlp.2.weight\", \"layers.11.w_mlp_out.gate_row.router.mlp.2.bias\", \"layers.11.w_mlp_out.gate_col.router.mlp.0.weight\", \"layers.11.w_mlp_out.gate_col.router.mlp.0.bias\", \"layers.11.w_mlp_out.gate_col.router.mlp.2.weight\", \"layers.11.w_mlp_out.gate_col.router.mlp.2.bias\", \"lm_head.gate_col.router.mlp.0.weight\", \"lm_head.gate_col.router.mlp.0.bias\", \"lm_head.gate_col.router.mlp.2.weight\", \"lm_head.gate_col.router.mlp.2.bias\". \n\tUnexpected key(s) in state_dict: \"layers.0.ln1.norms.2.weight\", \"layers.0.ln1.norms.2.bias\", \"layers.0.ln2.norms.2.weight\", \"layers.0.ln2.norms.2.bias\", \"layers.1.ln1.norms.2.weight\", \"layers.1.ln1.norms.2.bias\", \"layers.1.ln2.norms.2.weight\", \"layers.1.ln2.norms.2.bias\", \"layers.2.ln1.norms.2.weight\", \"layers.2.ln1.norms.2.bias\", \"layers.2.ln2.norms.2.weight\", \"layers.2.ln2.norms.2.bias\", \"layers.3.ln1.norms.2.weight\", \"layers.3.ln1.norms.2.bias\", \"layers.3.ln2.norms.2.weight\", \"layers.3.ln2.norms.2.bias\", \"layers.4.ln1.norms.2.weight\", \"layers.4.ln1.norms.2.bias\", \"layers.4.ln2.norms.2.weight\", \"layers.4.ln2.norms.2.bias\", \"layers.5.ln1.norms.2.weight\", \"layers.5.ln1.norms.2.bias\", \"layers.5.ln2.norms.2.weight\", \"layers.5.ln2.norms.2.bias\", \"layers.6.ln1.norms.2.weight\", \"layers.6.ln1.norms.2.bias\", \"layers.6.ln2.norms.2.weight\", \"layers.6.ln2.norms.2.bias\", \"layers.7.ln1.norms.2.weight\", \"layers.7.ln1.norms.2.bias\", \"layers.7.ln2.norms.2.weight\", \"layers.7.ln2.norms.2.bias\", \"layers.8.ln1.norms.2.weight\", \"layers.8.ln1.norms.2.bias\", \"layers.8.ln2.norms.2.weight\", \"layers.8.ln2.norms.2.bias\", \"layers.9.ln1.norms.2.weight\", \"layers.9.ln1.norms.2.bias\", \"layers.9.ln2.norms.2.weight\", \"layers.9.ln2.norms.2.bias\", \"layers.10.ln1.norms.2.weight\", \"layers.10.ln1.norms.2.bias\", \"layers.10.ln2.norms.2.weight\", \"layers.10.ln2.norms.2.bias\", \"layers.11.ln1.norms.2.weight\", \"layers.11.ln1.norms.2.bias\", \"layers.11.ln2.norms.2.weight\", \"layers.11.ln2.norms.2.bias\". \n\tsize mismatch for layers.0.w_q.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.w_q.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.w_k.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.w_k.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.w_v.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.w_v.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.w_o.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.w_o.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.w_mlp_in.gate_row.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.0.w_mlp_in.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.w_mlp_out.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.0.w_mlp_out.gate_col.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.1.w_q.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.w_q.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.w_k.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.w_k.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.w_v.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.w_v.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.w_o.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.w_o.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.w_mlp_in.gate_row.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.1.w_mlp_in.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.w_mlp_out.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.1.w_mlp_out.gate_col.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.2.w_q.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.w_q.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.w_k.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.w_k.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.w_v.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.w_v.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.w_o.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.w_o.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.w_mlp_in.gate_row.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.2.w_mlp_in.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.w_mlp_out.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.2.w_mlp_out.gate_col.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.3.w_q.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.w_q.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.w_k.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.w_k.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.w_v.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.w_v.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.w_o.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.w_o.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.w_mlp_in.gate_row.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.3.w_mlp_in.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.w_mlp_out.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.3.w_mlp_out.gate_col.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.4.w_q.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.w_q.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.w_k.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.w_k.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.w_v.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.w_v.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.w_o.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.w_o.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.w_mlp_in.gate_row.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.4.w_mlp_in.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.w_mlp_out.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.4.w_mlp_out.gate_col.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.5.w_q.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.w_q.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.w_k.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.w_k.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.w_v.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.w_v.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.w_o.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.w_o.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.w_mlp_in.gate_row.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.5.w_mlp_in.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.w_mlp_out.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.5.w_mlp_out.gate_col.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.6.w_q.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.6.w_q.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.6.w_k.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.6.w_k.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.6.w_v.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.6.w_v.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.6.w_o.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.6.w_o.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.6.w_mlp_in.gate_row.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.6.w_mlp_in.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.6.w_mlp_out.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.6.w_mlp_out.gate_col.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.7.w_q.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.7.w_q.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.7.w_k.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.7.w_k.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.7.w_v.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.7.w_v.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.7.w_o.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.7.w_o.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.7.w_mlp_in.gate_row.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.7.w_mlp_in.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.7.w_mlp_out.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.7.w_mlp_out.gate_col.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.8.w_q.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.8.w_q.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.8.w_k.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.8.w_k.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.8.w_v.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.8.w_v.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.8.w_o.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.8.w_o.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.8.w_mlp_in.gate_row.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.8.w_mlp_in.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.8.w_mlp_out.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.8.w_mlp_out.gate_col.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.9.w_q.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.9.w_q.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.9.w_k.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.9.w_k.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.9.w_v.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.9.w_v.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.9.w_o.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.9.w_o.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.9.w_mlp_in.gate_row.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.9.w_mlp_in.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.9.w_mlp_out.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.9.w_mlp_out.gate_col.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.10.w_q.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.10.w_q.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.10.w_k.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.10.w_k.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.10.w_v.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.10.w_v.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.10.w_o.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.10.w_o.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.10.w_mlp_in.gate_row.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.10.w_mlp_in.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.10.w_mlp_out.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.10.w_mlp_out.gate_col.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.11.w_q.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.11.w_q.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.11.w_k.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.11.w_k.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.11.w_v.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.11.w_v.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.11.w_o.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.11.w_o.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.11.w_mlp_in.gate_row.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for layers.11.w_mlp_in.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.11.w_mlp_out.gate_row.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for layers.11.w_mlp_out.gate_col.old_gate: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for lm_head.gate_col.old_gate: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([384])."
    }
  },
  "end_time": "2025-12-18T15:51:46.119271",
  "total_time_seconds": 634.3844769001007,
  "summary": {
    "task_a_baseline_ppl": 1.0047244300088636,
    "task_a_retention_ppl": 1.0054154834155689,
    "task_a_retention_ratio": 0.99931266882388,
    "task_b_ppl": 10.557013534922373,
    "model_params_initial": 914999424,
    "model_params_final": 255129600,
    "expansion_added_params": -659869824,
    "total_training_time_hours": 0.17621791025002798
  }
}