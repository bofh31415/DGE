# DGE Architecture: Scientific Log
This document tracks critical architectural insights, hypothesis validations, and implementation requirements for the Double Gate Extension (DGE) model.

---

## 2025-12-13: The Noise Injection Vulnerability in Additive Gating

### Observation
Upon expanding a pre-trained model (Task A trained) from `d_model=64` to `128` and training Task B, the performance on Task A dropped to 0.0% (Catastrophic Forgetting). This occurred despite:
1.  Freezing the backward mask for old weights.
2.  Initializing new gate parameters to negative values (`-5.0`) to theoretically close them.

### Root Cause Analysis
The DGE mechanism uses an additive gate structure: 
$$ G_{ij} = \sigma(r_i + c_j) $$

Where $r_i$ is the row gate (output dim) and $c_j$ is the column gate (input dim).
1.  **State before expansion**: For active "Old" weights, $r_i$ is typically large positive (open).
2.  **Expansion**: New inputs (columns) are added. We initialize new $c_j$ to `-5.0`.
3.  **The Cross-Term Problem**: At the intersection of **Old Rows** and **New Columns**:
    $$ G_{i, new} = \sigma(r_{old} + c_{new}) $$
    If $r_{old} \approx +5.0$ and $c_{new} = -5.0$, then $r+c \approx 0$.
    $$ \sigma(0) = 0.5 $$
    **The gate is 50% OPEN.**

If the new weights $W_{new}$ at this intersection are initialized randomly (e.g., Kaiming or Xavier initialization), they instantly inject effectively 50% of that random noise into the pre-trained output activations. This signal-to-noise ratio degradation destroys the existing manifold representation immediately.

### Critical Requirement: Zero-Initialization
- Added Unit Test `test_expansion_zero_init` which confirmed that random noise was indeed being introduced.
- Updated `dge_utils.py` to use `nn.init.constant_(new_layer.weight, 0.0)`.
- Verified 100% Identity Preservation immediately after expansion.
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
[2025-12-13 20:45:00] CRITICAL DISCOVERY: The Shared Gate Leakage
--------------------------------------------------------------------------------
PROBLEM:
Even with perfect Weight Masking ($G_{bwd} = 0$), we observed 100% Catastrophic Forgetting of Skill A after training Skill B.
Investigation revealed that while the *Weights* were frozen, the *Gates* ($g_{row}, g_{col}$) remained trainable.

MECHANISM OF FAILURE:
1. DoubleGateLinear uses shared gate vectors: $W_{eff} = W \odot \sigma(g_{row} + g_{col})$.
2. When expanding, we added new rows/cols but kept the old gate parameters as part of the new vectors.
3. During Skill B training, the optimizer updated $g_{row}$ and $g_{col}$ to minimize Loss B.
4. Since these gates are *shared* with the locked weights of Skill A, changing them altered the effective weights of Skill A ($W_{eff}^A$).
5. $W_{eff}^A$ drifted significantly, destroying the memory.

ADDITIONAL FACTOR: AdamW Weight Decay
1. AdamW applies decay to *all* parameters, even if their gradient is masked to 0.
2. This caused a slow "erosion" of the frozen static weights over time.

SOLUTION:
1. **Gate Freezing**: Implemented `gate_row_mask` and `gate_col_mask`.
   - We now strictly freeze the segments of the gate vectors corresponding to the old task.
   - Skill B can only change gates for *new* rows/cols or *new* interactions, not old ones.
2. **Disable Weight Decay**:
   - Set `weight_decay=0.0` for the optimizer.
   - We rely solely on Gradient Masking for regularization/freezing.

VERIFICATION:
- Unit Test `test_frozen_integrity_with_zero_decay` confirmed weight stability.
- Unit Test `test_gate_freezing` confirmed gate vector segment stability.
- Forensic Logging `Frozen_Grad_Norm` now tracks both weight and gate drifts.
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
[2025-12-13 20:55:00] CRITICAL DISCOVERY: LayerNorm Reset (The Paradox)
--------------------------------------------------------------------------------
PROBLEM:
Validation Run `dge_val_20251213_204116` showed:
- `Frozen_Grad_Norm` = 0.000 (Perfect Freezing)
- Skill A Accuracy = 0.03% (Total Failure)

ANALYSIS:
The forensic logs proved that the Core Weights and Gates were NOT modified by the optimizer.
Therefore, the function mapping failed due to a change in the *Context* or *Distribution* of activations.
Found: `DGEBlock.expand` was executing `self.ln1 = nn.LayerNorm(self.d_model)`.
This RE-INITIALIZES the LayerNorm, discarding the learned $\gamma$ (scale) and $\beta$ (shift) parameters.
Since the frozen core expects a specific input distribution (shaped by the old LN), feeding it un-scaled/un-shifted features results in garbage outputs.

SOLUTION:
Implement `expand_layer_norm` to copy the old $\gamma$ and $\beta$ to the valid indices of the new LayerNorm.
--------------------------------------------------------------------------------
**Solution**:
We must decouple the *potential* for flow (Gates) from the *actual* flow (Weights).
$$ W_{new\_physical} \leftarrow 0.0 $$

By strictly initializing the physical weights of the new capacity to zero:
$$ y = (W_{old} \odot G_{old})x + (0.0 \odot G_{new})x $$
$$ y = y_{old} + 0 $$

This restores perfect Identity Preservation ($f_{t+1}(x) = f_t(x)$) regardless of the gate states. Gradients can still flow into $W_{new}$ and $G_{new}$ to begin training, but the expansion starts from a neutral, non-destructive state.

### Conclusion
**Random initialization is forbidden for DGE expansion areas.** Zero-initialization is a hard requirement for the stability of the additive double-gate topology.

--------------------------------------------------------------------------------
[2025-12-13 21:00:00] CRITICAL DISCOVERY: Embedding Amnesia (The Final Leak)
--------------------------------------------------------------------------------
PROBLEM:
Validation Run `dge_val_20251213_205843` (after LayerNorm fix) showed:
- `Frozen_Grad_Norm` = 0.000 (Global Zero!)
- Skill A Accuracy = 0.06% (Still failing)

ANALYSIS:
If the core weights, gates, and LayerNorms are mathematically frozen (proven by grad norm), and the model still forgets, the only remaining variable is the **INPUT**.
In `DGESimpleTransformer`, inputs come from:
1. `token_emb` (nn.Embedding)
2. `pos_emb` (nn.Parameter)
These were being expanded by *copying* weights, but NOT *freezing* the old columns.
Since Skill B uses tokens that overlap with Skill A (e.g., numbers, common words), the optimizer updated the shared vector representations.
Input drift + Frozen Core = Garbage Output.

VERIFICATION:
- Created unit test `test_embedding_leakage`.
- Confirmed that without hooks, gradients flow into the "old" columns of an expanded embedding.

SOLUTION:
- Implemented `expand_embedding` and `expand_parameter` in `dge_utils.py`.
- These functions register a backward hook that physically masks gradients for the first $d_{old}$ columns.
- Re-ran `test_embedding_leakage`: Passed (Grad Norm 0.0).

STATUS:
- Version bumped to `V 0.1.0` (Testing Phase).
- All known leaks (Weight Decay, Gates, LayerNorms, Embeddings) are now patched.
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
[2025-12-13 21:15:00] V 0.1.1: The Dead Gradient (Initialization Bug)
--------------------------------------------------------------------------------
PROBLEM:
After fixing input leakage (V 0.1.0), Skill A retention improved (34%), but Skill B FAILED to learn (3% Accuracy).
Investigation of `training.csv` showed `Active_Grad_Norm` was healthy (~0.2), but the model wouldn't converge.

ROOT CAUSE:
In `expand_embedding`, I implemented Identity Preservation for new columns by setting them to `0.0`.
However, Gradients for an Embedding are proportional to the input vector value.
$$ \frac{\partial L}{\partial W} \propto \dots \times x $$
If $x = 0$, then Gradient = 0.
The new embedding parameters were technically trainable, but received zero signal because they were initialized to void.

FIX (V 0.1.1):
- Changed `expand_embedding` initialization for new columns to `nn.init.normal_(std=0.02)`.
- Use small random noise instead of zero.
   - But Hidden Layers are block-diagonal (Sidecar). Inputs to Old Core are frozen outputs of previous Old Core.
   - The entire A-path is physically insulated.

[2025-12-13 21:58:00] V 0.1.5 Result: CATASTROPHIC FAILURE (Paradox)
--------------------------------------------------------------------------------
Forensics were CLEAN (Frozen Grads = 0.000).
Initialization was PERFECT (ID Delta = 0.0).
Result:
- Count Up (Skill A): **2.65%** (DESTROYED)
- Count Down (Skill B): **40.53%** (LEARNING)

ANALYSIS:
We have a "Ghost in the Shell". parameters are frozen, but the function is destroyed.
How can $f(x)$ change if $W$ doesn't change?
Answer: The **Context** changed.

Suspect #1: **LayerNorm Coupling**
- Standard LN: $y = \frac{x - \mu}{\sigma} \gamma + \beta$
- Statistics $\mu, \sigma$ are computed over the *entire* dimension $d_{model}$.
- When we expanded: $x_{new} = [x_{old}, x_{sidecar}]$.
- Initially $x_{sidecar} \approx 0$, so $\mu, \sigma$ are unchanged.
- As Task B trains, $x_{sidecar}$ becomes active.
- This SHIFTS $\mu$ and $\sigma$ for the entire vector.
- The Old Core receives $x_{old}$ scaled by the *new* global statistics.
- This corrupts the amplitude/offset of the signal. The frozen weights $W_{old}$ receive garbage.

Suspect #2: **Softmax Competition**
- $P(A) = \frac{e^{A}}{e^{A} + e^{B}}$
- If $B$ logits grow large (even on A's inputs, due to bias leaks or cross-talk through LN), A's probability vanishes.

Next Step:
Verify LayerNorm hypothesis. We might need   - This CORRUPTS the "Frozen Core" functionally.
   - Fix: `SplitLayerNorm` to decouple statistics.

[2025-12-13 22:05:00] V 0.1.6 Result: THE FINAL BARRIER (Static Gating)
--------------------------------------------------------------------------------
Architecture: DGE with Block Diagonal + Frozen Bias + Split LayerNorm.
Forensics (Run `dge_val_20251213_215717`):
- `Frozen_Weight_Grad`: **0.000000000** (PASS)
- `Frozen_Bias_Grad`:   **0.000000000** (PASS)
- `Frozen_Head_Grad`:   **0.000000000** (PASS)
- `Frozen_Grad_Norm`:   **0.000000000** (PASS)

Parameters are absolutely frozen. Function implies isolated normalization.
Result:
- Skill A: **0.03%** (Acc < Random Chance).
- Skill B: **100.00%**.

ANALYSIS:
The failure is **Interference by Additive Superposition**.
Equation: $Y(x) = Y_{Old}(x) + Y_{New}(x)$
In our implementation, the Gates ($g_{row}, g_{col}$) are **STATIC Parameters**.
They are optimized once and fixed. They define a static topology (Sidecar Mode).
The Sidecar is **Always On** for any input that passes through the network.

Critically: **Skill A and Skill B share the same vocabulary (Token IDs)**.
When we input a sequence for Skill A (e.g., "1 2 3"), the *Embeddings* produce vectors.
Since the embeddings are shared (rows), the Sidecar receives this input.
The Sidecar processes it. Since the Sidecar was trained on Skill B ("Count Down"), it sees "1 2 3" and predicts "2" (Down) or output relevant to B.
$Y_{Old}$ predicts "4" (Up).
$Y_{New}$ predicts "2" (Down).
The Head sums them: $Logits = L_{Old} + L_{New}$.
Since Skill B is fresh and optimized (Loss 0.009), its logits $L_{New}$ are likely larger/sharper than $L_{Old}$.
Result: The model confidently predicts B's answer.

CONCLUSION:
We have hit the limit of "Static Expansion".
You cannot have two conflicting skills on the same domain (tokens) active simultaneously in an additive architecture.
We effectively built a "Schizo-Model" that hears two voices at once.

SOLUTION (V 0.2.0):
**Input-Dependent Gating (Mixture of Experts)**.
The Gates must be dynamic: $G(x) = \sigma(W_{gate} \cdot x)$.
If the input context looks like Skill A, $G_{New}(x) \to 0$.
If the input context looks like Skill B, $G_{New}(x) \to 1$.
This "Routing" is the only way to satisfy Reviewer Point 3.2.
----------------------------------------------------------------------------------
[2025-12-13 21:20:00] V 0.1.2: The Double-Lock (Vanishing Gradient)
--------------------------------------------------------------------------------
PROBLEM:
Even with noise initialization (V 0.1.1), Skill B performance remained poor (~3.5%).
Forensic Log showed `Active_Grad_Norm` dropping to extremely low levels (~0.04).

ROOT CAUSE:
The default DGE strategy for expansion was:
1. Initialize New Weights ($W_{new}$) to `0.0` (for Identity).
2. Initialize New Gates ($G_{new}$) to `-5.0` (Closed, for Identity).

This created a "Double-Lock":
- To update $W_{new}$, gradients must pass through the Gate ($Gradient \times \sigma(G)$). If Gate is closed ($\sigma \approx 0$), W doesn't learn.
- To update $G_{new}$, gradients must depend on the magnitude of $W_{new}$. If $W=0$, G doesn't learn.
Result: The new capacity was strangled.

FIX (V 0.1.2):
- **Open the Gates**: Changed `expand_dge_linear` gate initialization from `-5.0` to `2.5` (Open, $\sigma \approx 0.99$).
- **Keep Weights Zero**: $W_{new} = 0.0$.
- Effect:
    - Identity is Preserved: $y = W_{old}x + 0.0 \times 1.0 = y_{old}$.
    - Gradients Flow: The open gate allowing full gradient signal to reach $W_{new}$ immediately. $W_{new}$ moves away from 0, and learning begins.


--------------------------------------------------------------------------------
[2025-12-13 21:30:00] V 0.1.3: Block-Diagonal Isolation (The Head Leak)
--------------------------------------------------------------------------------
PROBLEM:
Implemented strict Block-Diagonal Isolation (`isolate_cross_terms=True`) for all hidden layers.
Results:
- `Frozen_Grad_Norm` = 0.000000000 (Hidden Layers are perfectly sealed).
- `Active_Grad_Norm` = High (Skill B learns well: 90%).
- Skill A Accuracy = 0.53% (Total Failure).

ANALYSIS:
If the Input is frozen (embedded hooks), and Hidden Layers are isolated (sidecar), the only remaining shared component is the **LM HEAD**.
$$ \text{Logits} = \text{LayerNorm}(h_{final}) \cdot W_{head}^T $$
$W_{head}$ has shape `[d_model, vocab_size]`.
During expansion, we increased `d_model` (added columns to $W_{head}$).
However, the `vocab_size` (rows of the output matrix) remains constant.
Both Skill A and Skill B predict tokens from the same vocabulary.
When training Skill B:
1. $h_{final}$ contains valid B-features and (allegedly) valid A-features.
2. The optimizer updates $W_{head}$ to map B-features to B-tokens.
3. Crucially, if B uses token "7", it updates the embedding vector for "7" in $W_{head}$.
4. This overwrites the vector that Skill A relied on to predict "7".
Since there is no "New 7" vs "Old 7" in a shared vocabulary, the output representations are destructive.

CONCLUSION:
**The Shared Head is the bottleneck.**
Block-Diagonal Hidden Layers are insufficient if they converge into a destructive Shared Head.
We must isolate the Readout Layer.

PROPOSAL (V 0.2.0):
We need **Projected Head Expansion** or **Task-Specific Heads**.
Since DGE aims to be a single model:
- We can expand the Head's input dimension (done).
- We must mask the *gradients* to the Old Head Columns? No, that freezes the readout for A, which is good.
- Wait, if we freeze Old Head Columns, A is safe.
- Did `expand_dge_linear` isolate the Head?
- The Head is a standard `nn.Linear` (usually), or `DGE`?
- In `DGEBlock`, we used `expand_dge_linear`.
- In `DGESimpleTransformer`, `self.lm_head` is an `nn.Linear`.
- `expand_model` does:
```python
new_head = nn.Linear(new_full_dim, vocab_size, bias=False)
new_head.weight.data[:, :old_dim] = old_head.weight.data
```
**IT DOES NOT REGISTER HOOKS TO FREEZE THE OLD COLUMNS OF THE HEAD.**
The Head is a standard Linear layer (not DGE). It has no `backward_mask`.
Therefore, gradients from B flow into `head.weight[:, :old_dim]`, destroying A's readout.

FIX:
Apply `register_hook` to the Head Weight to freeze the old columns (similar to Embeddings).
--------------------------------------------------------------------------------
[2025-12-13 21:40:00] V 0.1.4 -> V 0.1.5: The Hidden Bias Leak
--------------------------------------------------------------------------------
PROBLEM:
Ran V 0.1.4 (Head Isolation).
Results:
- Skill A Accuracy = 0.12% (Still Failing).
- Head was isolated. Embeddings were isolated. Weights/Gates were isolated.

ANALYSIS:
Deep dive into `dge_utils.py` revealed that `expand_dge_linear`:
1. Copies the `bias` from Old to New.
2. Registers hooks for `weight`, `gate_row`, `gate_col`.
3. **DOES NOT register a hook for `bias`.**

Consequence:
When we expand the Output Dimension (Sidecar Mode), we just append new rows.
The `bias` vector grows. `bias[0:old_out]` belongs to Skill A's block.
Skill B's gradients flow through the ENTIRE layer (via `bias` vector summation in `F.linear`).
Since `bias` had no hook, Skill B optimized the *entire* bias vector, shifting the activation baseline for Skill A's frozen block.
This corrupted the features before they even reached the isolated Head.

FIX (V 0.1.5):
Added explicit Bias Freezing logic to `expand_dge_linear`.
Now registers `frozen_bias_mask` and a hook on `new_layer.bias`.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
